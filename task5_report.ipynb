{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e25d70c-9bc7-472e-b389-f3f5bd138cab",
   "metadata": {},
   "source": [
    "## Understanding Logistic Regression (Detailed Explanation)\n",
    "\n",
    "Logistic Regression is a **supervised machine learning algorithm** primarily used for\n",
    "**binary classification problems**, where the output variable has two possible classes,\n",
    "such as *yes/no*, *true/false*, or *disease/no disease*.\n",
    "\n",
    "Despite its name, Logistic Regression is a **classification algorithm**, not a regression\n",
    "algorithm. The term “regression” comes from the fact that the model computes a linear\n",
    "combination of input features before applying a nonlinear transformation.\n",
    "\n",
    "### 1. Linear Model Component\n",
    "\n",
    "Logistic Regression starts by computing a **linear combination** of input features:\n",
    "\n",
    "![](https://miro.medium.com/1*_TqRJ9SmwFzRigJhMiN2uw.png)\n",
    "\n",
    "This linear equation is similar to linear regression, but **its output is not used directly**\n",
    "for prediction.\n",
    "\n",
    "### 2. Sigmoid Function (Core of Logistic Regression)\n",
    "\n",
    "The linear output \\(z\\) can take any real value (positive or negative).  \n",
    "To convert this value into a **probability**, Logistic Regression uses the **sigmoid function**:\n",
    "\n",
    "![Sigmoid Function](https://d1.awsstatic.com/sigmoid.bfc853980146c5868a496eafea4fb79907675f44.png)\n",
    "\n",
    "The sigmoid function maps any real number into the range **0 to 1**, making it ideal for\n",
    "probability estimation.\n",
    "\n",
    "#### Key properties of the sigmoid function:\n",
    "- Large negative values → output close to **0**\n",
    "- Large positive values → output close to **1**\n",
    "- z = 0 → output = **0.5**\n",
    "\n",
    "**Sigmoid Function Graph (Reference):**  \n",
    "![Sigmoid Function Graph](https://media.licdn.com/dms/image/v2/D4D12AQGIXdSG7IJCNw/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1694183259537?e=2147483647&v=beta&t=lJ_qEzot0iGYhNpez9XGRNHjS-CDKHn3Wj-6iCQxRO0)\n",
    "\n",
    "This S-shaped curve explains how logistic regression smoothly transitions between\n",
    "class predictions instead of making abrupt decisions.\n",
    "\n",
    "### 3. Probability Interpretation\n",
    "\n",
    "The output of the sigmoid function is interpreted as:\n",
    "\n",
    "\\[\n",
    "P(y = 1 | x)\n",
    "\\]\n",
    "\n",
    "- If probability ≥ 0.5 → class **1**\n",
    "- If probability < 0.5 → class **0**\n",
    "\n",
    "This threshold can be adjusted depending on the problem, especially in medical\n",
    "applications where recall is critical.\n",
    "\n",
    "### 4. Loss Function: Binary Cross-Entropy\n",
    "\n",
    "To train the model, Logistic Regression minimizes the **binary cross-entropy loss**:\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*dEZxrHeNGlhfNt-JyRLpig.png)\n",
    "\n",
    "This loss function penalizes:\n",
    "- Confident wrong predictions heavily\n",
    "- Correct predictions lightly\n",
    "\n",
    "**Loss Function Visualization:**  \n",
    "![](https://www.loekvandenouweland.com/assets/logloss/logloss.jpg)\n",
    "\n",
    "### 5. Gradient Descent Optimization\n",
    "\n",
    "Logistic Regression uses **gradient descent** to minimize the loss function.\n",
    "\n",
    "Steps involved:\n",
    "1. Initialize weights and bias\n",
    "2. Compute predictions using sigmoid\n",
    "3. Calculate loss\n",
    "4. Compute gradients\n",
    "5. Update parameters\n",
    "6. Repeat until convergence\n",
    "\n",
    "Over time, the loss decreases, indicating improved predictions.\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/0*QGWG1SbMjNjQ2wWk.png)\n",
    "A downward-sloping curve confirms that the model is learning effectively.\n",
    "\n",
    "### 6. Decision Boundary\n",
    "\n",
    "Logistic Regression creates a **linear decision boundary** that separates the two classes.\n",
    "In higher dimensions, this boundary becomes a hyperplane.\n",
    "\n",
    "Decision Boundary Visualization:**  \n",
    "![](https://2.bp.blogspot.com/-eCQJ4j3wqw0/Tr8-bwNir1I/AAAAAAAAApg/mBLTGATfgI0/s1600/plot3.png)\n",
    "\n",
    "### 7. Importance of Feature Scaling\n",
    "\n",
    "Since gradient descent relies on distance and magnitude, features with large values\n",
    "can dominate the learning process.\n",
    "\n",
    "Standardization ensures:\n",
    "- Faster convergence\n",
    "- Stable gradients\n",
    "- Fair weight updates\n",
    "\n",
    "This is why **StandardScaler** is used before training.\n",
    "\n",
    "## Dataset Used\n",
    "\n",
    "The **Framingham Heart Study dataset** was used for this task to predict the likelihood\n",
    "of developing coronary heart disease within 10 years.\n",
    "\n",
    "- **Target Variable:** `TenYearCHD`\n",
    "  - 0 → No heart disease\n",
    "  - 1 → Heart disease\n",
    "- **Features:** Demographic, lifestyle, and medical attributes\n",
    "- **Nature of problem:** Binary classification\n",
    "\n",
    "This dataset is well-suited for Logistic Regression due to its probabilistic interpretation\n",
    "and medical relevance.\n",
    "\n",
    "## Approach Followed to Solve the Task\n",
    "\n",
    "### Logistic Regression from Scratch\n",
    "\n",
    "The scratch implementation involved manually coding:\n",
    "- Sigmoid function\n",
    "- Weight and bias initialization\n",
    "- Gradient descent\n",
    "- Binary prediction logic\n",
    "\n",
    "This approach helped in understanding:\n",
    "- How probabilities are computed\n",
    "- How loss is minimized\n",
    "- How model parameters evolve over time\n",
    "\n",
    "### Logistic Regression using scikit-learn\n",
    "\n",
    "Scikit-learn’s implementation abstracts the mathematical complexity and provides:\n",
    "- Optimized solvers\n",
    "- Built-in regularization\n",
    "- Faster and more stable convergence\n",
    "\n",
    "This model served as a **benchmark** for performance comparison.\n",
    "\n",
    "## Evaluation Metrics Used\n",
    "\n",
    "Since the problem is a **binary classification task**, the following metrics were used:\n",
    "\n",
    "- **Accuracy** – overall correctness\n",
    "- **Precision** – reliability of positive predictions\n",
    "- **Recall** – ability to detect actual heart disease cases\n",
    "- **F1-score** – balance between precision and recall\n",
    "\n",
    "Using multiple metrics ensured a well-rounded evaluation.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Logistic Regression is a powerful and interpretable classification algorithm,\n",
    "especially suitable for medical decision-making.\n",
    "\n",
    "Implementing it from scratch builds strong conceptual clarity, while using\n",
    "scikit-learn demonstrates how machine learning is applied efficiently in practice.\n",
    "\n",
    "This task reinforced the importance of understanding both **algorithm internals**\n",
    "and **high-level ML tools** for building reliable predictive models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
