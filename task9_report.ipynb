{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead9286a-28d5-4717-8b6e-527bdf995352",
   "metadata": {},
   "source": [
    "# TASK 9: Evaluation Metrics – Pick the Best Performer\n",
    "\n",
    "## Description of the Task\n",
    "\n",
    "The objective of this task is to evaluate and compare multiple **pretrained machine learning models**\n",
    "using a suitable test dataset and identify the best-performing model based on standard evaluation metrics.\n",
    "\n",
    "Unlike typical machine learning tasks that focus on training models, this task emphasizes\n",
    "**model evaluation and comparison**. It helps in understanding how different algorithms behave\n",
    "when tested on the same data and how evaluation metrics guide the selection of the most effective model.\n",
    "\n",
    "Specifically, the task involves:\n",
    "\n",
    "- Selecting an appropriate test dataset  \n",
    "- Loading pretrained models saved as `.pkl` files  \n",
    "- Using the models to generate predictions on the test dataset  \n",
    "- Evaluating model performance using classification metrics  \n",
    "- Comparing results and identifying the best-performing model  \n",
    "\n",
    "## Understanding Model Evaluation\n",
    "\n",
    "Model evaluation is a critical step in the machine learning workflow, as it measures how well\n",
    "a trained model performs on unseen data.\n",
    "\n",
    "Rather than relying on a single metric, multiple evaluation metrics are used to gain a more\n",
    "comprehensive understanding of a model’s strengths and weaknesses. This task highlights the\n",
    "importance of selecting evaluation metrics based on the type of problem being solved.\n",
    "\n",
    "Since all the models provided were already trained, evaluation became the primary method\n",
    "for judging their effectiveness.\n",
    "\n",
    "## Dataset Used as Test Dataset\n",
    "\n",
    "I used the **Iris dataset** as the **test dataset** for evaluating the pretrained models.\n",
    "\n",
    "- **Target variable:** `Species`  \n",
    "  - Iris-setosa  \n",
    "  - Iris-versicolor  \n",
    "  - Iris-virginica  \n",
    "- **Features:**  \n",
    "  - Sepal length  \n",
    "  - Sepal width  \n",
    "  - Petal length  \n",
    "  - Petal width  \n",
    "- **Missing values:** None  \n",
    "\n",
    "The dataset was chosen because it is clean, balanced, and widely used for benchmarking\n",
    "classification models.\n",
    "\n",
    "## Pretrained Models Used\n",
    "\n",
    "The following five pretrained machine learning models were provided and evaluated:\n",
    "\n",
    "1. Decision Tree Classifier  \n",
    "2. Logistic Regression  \n",
    "3. K-Nearest Neighbors (KNN)  \n",
    "4. Support Vector Machine (SVM)  \n",
    "5. Random Forest Classifier  \n",
    "\n",
    "All models were loaded using the **joblib** library and evaluated on the same test dataset\n",
    "to ensure a fair and consistent comparison.\n",
    "\n",
    "## Approach Followed to Solve the Task\n",
    "\n",
    "### 1. Data Preparation\n",
    "\n",
    "I first loaded the Iris dataset into a Pandas DataFrame to inspect its structure and contents.\n",
    "The `Id` column was removed since it serves only as an identifier and does not contribute\n",
    "to model prediction.\n",
    "\n",
    "The remaining columns were then separated into:\n",
    "- **Features (X)** – input variables  \n",
    "- **Target (y)** – flower species  \n",
    "\n",
    "### 2. Handling Label Encoding\n",
    "\n",
    "During evaluation, I observed that some pretrained models produced **numerical class labels**,\n",
    "while the dataset contained **string-based class labels**.\n",
    "\n",
    "To ensure consistency between true labels and predicted labels, I applied\n",
    "**LabelEncoder** to encode the target variable into numerical form.\n",
    "This step was necessary to avoid errors during metric computation and ensure\n",
    "accurate evaluation.\n",
    "\n",
    "### 3. Model Evaluation\n",
    "\n",
    "Each pretrained model was used to generate predictions on the Iris test dataset.\n",
    "The predicted values were then compared with the true labels using multiple\n",
    "classification metrics.\n",
    "\n",
    "Evaluating all models on the **same test dataset** ensured consistency and enabled\n",
    "a meaningful comparison of performance across different algorithms.\n",
    "\n",
    "## Evaluation Metrics Used\n",
    "\n",
    "Since this is a **multi-class classification problem**, the following evaluation\n",
    "metrics were used:\n",
    "\n",
    "- **Accuracy** – measures overall correctness of predictions  \n",
    "- **Precision (weighted)** – evaluates the reliability of predicted classes  \n",
    "- **Recall (weighted)** – measures how well actual classes are identified  \n",
    "- **F1-score (weighted)** – provides a balance between precision and recall  \n",
    "\n",
    "Weighted averaging was used to ensure that all three Iris species were fairly\n",
    "represented in the evaluation.\n",
    "\n",
    "## Results and Model Comparison\n",
    "\n",
    "The evaluation results for all five models were compiled into a comparison table.\n",
    "This made it easy to observe differences in performance across models and metrics.\n",
    "\n",
    "The model with the **highest weighted F1-score** was identified as the\n",
    "best-performing model, as the F1-score provides a balanced measure of\n",
    "classification performance.\n",
    "\n",
    "## Difficulties Faced During the Task\n",
    "\n",
    "While completing this task, I encountered several practical challenges:\n",
    "\n",
    "- File path issues occurred while loading the pretrained model files, as they were\n",
    "  stored in a different directory. This was resolved by identifying and using\n",
    "  absolute file paths.\n",
    "- A mismatch in label formats caused errors during metric calculation, since the\n",
    "  dataset contained string labels while some models produced numerical predictions.\n",
    "  This issue was resolved using label encoding.\n",
    "- Warnings related to scikit-learn version differences and feature names were observed.\n",
    "  After verifying that these warnings did not affect prediction results, the evaluation\n",
    "  was carried out safely.\n",
    "\n",
    "## Outcomes and Learnings\n",
    "\n",
    "Through this task, I gained a strong understanding of how **pretrained machine learning models**\n",
    "can be evaluated and compared using appropriate evaluation metrics.\n",
    "\n",
    "I learned the importance of:\n",
    "- Selecting a suitable test dataset  \n",
    "- Ensuring consistency in preprocessing and label formats  \n",
    "- Choosing evaluation metrics based on the nature of the problem  \n",
    "\n",
    "The task also improved my debugging skills by exposing me to practical issues such as\n",
    "file handling, label mismatches, and library warnings.\n",
    "\n",
    "Overall, this task reinforced the idea that **model evaluation is just as important as\n",
    "model training** when building reliable and effective machine learning systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca17317-8acb-4464-98fe-e07351b6c332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
