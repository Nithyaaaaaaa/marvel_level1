{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead9286a-28d5-4717-8b6e-527bdf995352",
   "metadata": {},
   "source": [
    "# TASK 9: Evaluation Metrics – Pick the Best Performer\n",
    "\n",
    "## Description of the Task\n",
    "\n",
    "The objective of this task is to evaluate and compare multiple **pretrained machine learning models**\n",
    "using a suitable test dataset and identify the best-performing model based on standard evaluation metrics.\n",
    "\n",
    "Unlike typical machine learning tasks that focus on training models, this task emphasizes\n",
    "**model evaluation and comparison**. It helps in understanding how different algorithms behave\n",
    "when tested on the same data and how evaluation metrics guide the selection of the most effective model.\n",
    "\n",
    "Specifically, the task involves:\n",
    "\n",
    "- Selecting an appropriate test dataset\n",
    "- Loading pretrained models saved as `.pkl` files\n",
    "- Using the models to generate predictions on the test dataset\n",
    "- Evaluating model performance using classification metrics\n",
    "- Comparing results and identifying the best-performing model\n",
    "\n",
    "\n",
    "## Understanding Model Evaluation\n",
    "\n",
    "Model evaluation is a crucial step in the machine learning workflow.\n",
    "It helps measure how well a trained model performs on unseen data.\n",
    "\n",
    "Instead of relying on a single metric, multiple evaluation metrics are used to gain a more\n",
    "complete understanding of a model’s strengths and weaknesses. This task highlights the importance\n",
    "of choosing the right metrics based on the problem type.\n",
    "\n",
    "Since the models in this task are already trained, evaluation becomes the primary tool\n",
    "for judging their effectiveness.\n",
    "\n",
    "## Dataset Used as Test Dataset\n",
    "\n",
    "I used the **Iris dataset** as the **test dataset** for evaluation.\n",
    "\n",
    "- **Target variable:** `Species`\n",
    "  - Iris-setosa  \n",
    "  - Iris-versicolor  \n",
    "  - Iris-virginica  \n",
    "- **Features:**  \n",
    "  - Sepal length  \n",
    "  - Sepal width  \n",
    "  - Petal length  \n",
    "  - Petal width  \n",
    "- **Missing values:** None  \n",
    "\n",
    "The dataset was chosen because it is clean, balanced, and commonly used for benchmarking\n",
    "classification models.\n",
    "\n",
    "## Pretrained Models Used\n",
    "\n",
    "The following five pretrained machine learning models were provided and evaluated:\n",
    "\n",
    "1. Decision Tree Classifier  \n",
    "2. Logistic Regression  \n",
    "3. K-Nearest Neighbors (KNN)  \n",
    "4. Support Vector Machine (SVM)  \n",
    "5. Random Forest Classifier  \n",
    "\n",
    "All models were loaded using the **joblib** library and evaluated on the same test dataset\n",
    "to ensure a fair comparison.\n",
    "\n",
    "\n",
    "## Approach Followed to Solve the Task\n",
    "\n",
    "### 1. Data Preparation\n",
    "\n",
    "The Iris dataset was loaded into a Pandas DataFrame to inspect its structure.\n",
    "The `Id` column was removed since it is only an identifier and does not contribute to prediction.\n",
    "The remaining columns were separated into:\n",
    "\n",
    "- **Features (X)** – input variables\n",
    "- **Target (y)** – flower species\n",
    "\n",
    "\n",
    "### 2. Handling Label Encoding\n",
    "\n",
    "While evaluating the models, it was observed that some pretrained models returned\n",
    "numerical class labels, whereas the dataset contained class labels in string format.\n",
    "\n",
    "To ensure consistency between the true labels and predicted labels, **LabelEncoder**\n",
    "was used to encode the target variable into numerical form.\n",
    "This step was necessary to avoid errors during metric computation.\n",
    "\n",
    "\n",
    "### 3. Model Evaluation\n",
    "\n",
    "Each pretrained model was used to generate predictions on the Iris test dataset.\n",
    "The predicted values were then compared with the true labels using multiple evaluation metrics.\n",
    "\n",
    "Evaluating all models on the same dataset ensured consistency and allowed for\n",
    "a meaningful comparison of performance.\n",
    "\n",
    "\n",
    "## Evaluation Metrics Used\n",
    "\n",
    "Since this is a **multi-class classification problem**, the following metrics were used:\n",
    "\n",
    "- **Accuracy** – measures overall correctness of predictions  \n",
    "- **Precision (weighted)** – evaluates how reliable the predicted classes are  \n",
    "- **Recall (weighted)** – measures how well actual classes are identified  \n",
    "- **F1-score (weighted)** – provides a balance between precision and recall  \n",
    "\n",
    "Weighted averaging was used so that all three Iris species were fairly represented\n",
    "in the evaluation.\n",
    "\n",
    "\n",
    "## Results and Model Comparison\n",
    "\n",
    "The evaluation results for all five models were compiled into a comparison table.\n",
    "This made it easy to observe differences in performance across models and metrics.\n",
    "\n",
    "The model with the **highest weighted F1-score** was identified as the best-performing model,\n",
    "as F1-score provides a balanced measure of classification performance.\n",
    "\n",
    "\n",
    "## Difficulties Faced During the Task\n",
    "\n",
    "Several challenges were encountered while completing this task:\n",
    "\n",
    "- File path issues occurred while loading the pretrained model files, as they were stored\n",
    "  in a different directory. This was resolved by identifying and using absolute file paths.\n",
    "- A label format mismatch caused errors during metric calculation because the dataset\n",
    "  contained string labels while some models produced numerical predictions.\n",
    "  This issue was resolved using label encoding.\n",
    "- Warnings related to scikit-learn version differences and feature names were encountered.\n",
    "  After understanding that these warnings did not affect prediction results, the evaluation\n",
    "  was carried out safely.\n",
    "\n",
    "These challenges helped highlight common real-world issues that arise during model evaluation.\n",
    "\n",
    "## Outcomes and Learnings\n",
    "\n",
    "Through this task, I gained a strong understanding of how pretrained machine learning models\n",
    "can be evaluated and compared using appropriate metrics.\n",
    "\n",
    "I learned the importance of selecting a suitable test dataset, ensuring consistent preprocessing,\n",
    "and choosing the right evaluation metrics based on the problem type. The task also improved my\n",
    "debugging skills by exposing me to practical issues such as file handling, label mismatches,\n",
    "and library warnings.\n",
    "\n",
    "Overall, this task reinforced the idea that model evaluation is just as important as model training\n",
    "in building reliable machine learning systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
